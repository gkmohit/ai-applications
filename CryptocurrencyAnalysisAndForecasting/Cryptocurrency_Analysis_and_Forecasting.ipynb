{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4909d470",
   "metadata": {},
   "source": [
    "# Cryptocurrency Analysis and Forecasting\n",
    "## Machine Learning Research Project by Mohit Kishore\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This is a comprehensive **research project** exploring the application of machine learning and statistical techniques to cryptocurrency market analysis. The notebook performs end-to-end cryptocurrency data analysis, including exploratory data analysis, technical analysis, and comparative machine learning model evaluation.\n",
    "\n",
    "**Note**: This project is for **educational and research purposes only**. It demonstrates AI applications in quantitative finance but should not be used for actual investment decisions.\n",
    "\n",
    "### Project Goals\n",
    "\n",
    "The primary goals of this project are:\n",
    "\n",
    "1. **Data Exploration** - Understand cryptocurrency market data, price patterns, and trading volumes\n",
    "2. **Technical Analysis** - Identify patterns using moving averages, volatility metrics, and seasonality analysis\n",
    "3. **Model Comparison** - Evaluate 5 different ML algorithms on the same problem to understand their strengths\n",
    "4. **Portfolio Optimization** - Apply Modern Portfolio Theory to find optimal asset allocations\n",
    "5. **Forecasting** - Build time series models to predict future cryptocurrency prices\n",
    "6. **Research** - Answer practical questions about cryptocurrency market behavior\n",
    "\n",
    "### Machine Learning Algorithms\n",
    "\n",
    "We compare **5 different ML algorithms** to forecast cryptocurrency prices:\n",
    "\n",
    "| Algorithm | Type | Purpose | Complexity |\n",
    "|-----------|------|---------|------------|\n",
    "| **ARIMA** | Statistical | Univariate time series forecasting | Low |\n",
    "| **XGBoost** | Gradient Boosting | Feature-based prediction with technical indicators | Medium |\n",
    "| **LSTM** | Deep Learning | Capture temporal dependencies in sequences | High |\n",
    "| **GRU** | Deep Learning | Similar to LSTM with fewer parameters | High |\n",
    "| **TimeGPT/Prophet** | Foundation Model | Transfer learning approach for robust forecasting | Medium |\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "This project attempts to answer 8 key questions about cryptocurrency markets:\n",
    "\n",
    "1. **Can we predict cryptocurrency prices** using historical patterns and machine learning?\n",
    "2. **Which cryptocurrencies have the best risk-adjusted returns** (Sharpe ratio)?\n",
    "3. **Do moving average strategies work** in volatile crypto markets?\n",
    "4. **How correlated are different cryptocurrencies** and can we diversify effectively?\n",
    "5. **What is the optimal portfolio allocation** using historical data?\n",
    "6. **Can historical volatility predict future price movements?**\n",
    "7. **Do altcoins follow Bitcoin's trends** or move independently?\n",
    "8. **What seasonal patterns exist** in cryptocurrency returns?\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Data Loading** - Download top 100 cryptocurrencies from Kaggle\n",
    "2. **Cryptocurrency Selection** - Choose which crypto to analyze\n",
    "3. **EDA** - Visualize price trends, volume, returns distributions\n",
    "4. **Technical Analysis** - Moving averages, volatility, seasonality\n",
    "5. **Feature Engineering** - Create technical indicators\n",
    "6. **Model Training** - Train all 5 algorithms\n",
    "7. **Model Evaluation** - Compare performance metrics\n",
    "8. **Results Analysis** - Generate insights and final report\n",
    "\n",
    "### Author\n",
    "\n",
    "**Mohit Kishore**\n",
    "Website: https://www.mohitkishore.com\n",
    "\n",
    "This notebook is part of my AI Applications research workspace, exploring practical implementations of machine learning across different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442d149",
   "metadata": {},
   "source": [
    "## Section 1: Google Drive Connection and Setup\n",
    "\n",
    "This section establishes a connection to Google Drive to store all analysis outputs in the cloud. The code prompts users to decide whether to use Google Drive or local storage, then creates the necessary directories for saving visualizations and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e892a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/gkmohit/Development/Projects/AIApplications/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "import json\n",
    "\n",
    "try:\n",
    "    connect_drive = input(\"Do you want to connect to Google Drive to store outputs? (yes/no): \").strip().lower()\n",
    "    \n",
    "    if connect_drive == 'yes':\n",
    "        drive.mount('/content/drive')\n",
    "        output_dir = '/content/drive/MyDrive/Crypto_Analysis_Output'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        print(f\"Google Drive connected. Outputs will be saved to: {output_dir}\")\n",
    "    else:\n",
    "        output_dir = '/content/Crypto_Analysis_Output'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        print(f\"Outputs will be saved locally to: {output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up storage: {e}\")\n",
    "    output_dir = '/content/Crypto_Analysis_Output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Using local storage: {output_dir}\")\n",
    "\n",
    "# Configure plot saving\n",
    "save_plots = True  # Set to False to skip saving plots\n",
    "print(f\"Plot saving: {'Enabled' if save_plots else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645d020",
   "metadata": {},
   "source": [
    "## Section 2: Kaggle Dataset Download and Setup\n",
    "\n",
    "This section handles the download of cryptocurrency data from Kaggle. It prompts users to upload their kaggle.json authentication file, configures the Kaggle API, and installs all necessary Python packages for the analysis including deep learning frameworks and time series libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "try:\n",
    "    print(\"=\"*60)\n",
    "    print(\"KAGGLE API SETUP\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nPlease upload your kaggle.json file from Kaggle settings:\")\n",
    "    print(\"1. Go to: https://www.kaggle.com/settings/account\")\n",
    "    print(\"2. Click 'Create New API Token'\")\n",
    "    print(\"3. Upload the downloaded kaggle.json file below\\n\")\n",
    "\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if 'kaggle.json' in uploaded:\n",
    "        os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "        with open(os.path.expanduser('~/.kaggle/kaggle.json'), 'w') as f:\n",
    "            # Decode bytes to string and parse/rewrite as JSON\n",
    "            content = json.loads(uploaded['kaggle.json'].decode('utf-8'))\n",
    "            f.write(json.dumps(content, indent=2))\n",
    "        os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
    "        print(\"✓ kaggle.json configured successfully!\")\n",
    "        print(\"  Username:\", content.get('username', 'N/A'))\n",
    "    else:\n",
    "        print(\"✗ kaggle.json not found. Please upload the file.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error configuring Kaggle: {e}\")\n",
    "    print(\"  Please try uploading the file again.\")\n",
    "\n",
    "# Install required packages\n",
    "try:\n",
    "    print(\"\\nInstalling required packages...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\", \"torch\", \"tensorflow\", \"prophet\", \"nixtla\"], check=False)\n",
    "    print(\"✓ Packages installed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error installing packages: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ec204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "\n",
    "try:\n",
    "    dataset_path = \"/content/crypto_dataset\"\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    print(\"Downloading cryptocurrency dataset...\")\n",
    "    kaggle.api.dataset_download_files(\n",
    "        'mihikaajayjadhav/top-100-cryptocurrencies-daily-price-data-2025',\n",
    "        path=dataset_path,\n",
    "        unzip=True\n",
    "    )\n",
    "    print(f\"Dataset downloaded to {dataset_path}\")\n",
    "\n",
    "    downloaded_files = os.listdir(dataset_path)\n",
    "    print(f\"\\nDownloaded files (first 10):\")\n",
    "    for file in downloaded_files[:10]:\n",
    "        print(f\"  - {file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"Please ensure your Kaggle API is properly configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac277611",
   "metadata": {},
   "source": [
    "## Section 3: Import Libraries and Load Data\n",
    "\n",
    "This section imports all necessary libraries for data analysis, visualization, and machine learning. It then loads cryptocurrency data from CSV files into a dictionary structure, standardizing column names and sorting by date for consistent data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885038c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "crypto_data = {}\n",
    "\n",
    "try:\n",
    "    print(\"Loading cryptocurrency data...\")\n",
    "    # ONLY load the daily historical CSV file with date and price data\n",
    "    daily_file = f'{dataset_path}/crypto_historical_365days.csv'\n",
    "    \n",
    "    if os.path.exists(daily_file):\n",
    "        try:\n",
    "            df = pd.read_csv(daily_file)\n",
    "            df.columns = df.columns.str.lower().str.strip()\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date')\n",
    "            \n",
    "            # Extract individual cryptocurrencies from the daily file\n",
    "            if 'coin_name' in df.columns:\n",
    "                for coin_name in sorted(df['coin_name'].unique()):\n",
    "                    coin_df = df[df['coin_name'] == coin_name].copy().reset_index(drop=True)\n",
    "                    crypto_key = coin_name.lower().replace(' ', '_').strip()\n",
    "                    crypto_data[crypto_key] = coin_df\n",
    "                    print(f\"  Loaded {coin_name.title()}\")\n",
    "            else:\n",
    "                crypto_data['daily_data'] = df\n",
    "                print(f\"  Loaded daily cryptocurrency data\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading daily file: {str(e)[:50]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {filename}: {str(e)[:50]}\")\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded {len(crypto_data)} cryptocurrencies\")\n",
    "    if len(crypto_data) > 0:\n",
    "        print(f\"Cryptocurrencies: {', '.join(list(crypto_data.keys())[:10])}...\")\n",
    "    else:\n",
    "        print(\"No data files found. Please check dataset download.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data loading process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'bitcoin' in crypto_data:\n",
    "        btc = crypto_data['bitcoin']\n",
    "        print(\"Bitcoin Dataset Overview:\")\n",
    "        print(f\"Shape: {btc.shape}\")\n",
    "        print(f\"\\nColumns: {list(btc.columns)}\")\n",
    "        print(f\"\\nData Types:\\n{btc.dtypes}\")\n",
    "        print(f\"\\nMissing Values:\\n{btc.isnull().sum()}\")\n",
    "        print(f\"\\nBasic Statistics:\\n{btc.describe()}\")\n",
    "        if 'date' in btc.columns:\n",
    "            print(f\"\\nDate Range: {btc['date'].min()} to {btc['date'].max()}\")\n",
    "    else:\n",
    "        print(\"Bitcoin data not available in loaded datasets.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying Bitcoin overview: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c24be",
   "metadata": {},
   "source": [
    "## Select Cryptocurrency for Analysis\n",
    "\n",
    "Choose which cryptocurrency to analyze for price prediction and detailed modeling. All analysis sections will use your selected cryptocurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a370442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data structure\n",
    "import os\n",
    "\n",
    "csv_files = sorted([f for f in os.listdir(dataset_path) if f.endswith('.csv')])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPLORING DATA STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"\\nFile: {csv_file}\")\n",
    "    file_path = os.path.join(dataset_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"   Shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    print(f\"   First row sample:\")\n",
    "    for col in df.columns[:6]:\n",
    "        print(f\"     {col}: {df[col].iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb04f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load all cryptocurrency data from CSV files\n",
    "    crypto_data = {}\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING CRYPTOCURRENCY DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"[WARNING] Dataset path not found: {dataset_path}\")\n",
    "        print(\"Attempting to use local dataset folder...\\n\")\n",
    "        if os.path.exists(\"./dataset\"):\n",
    "            dataset_path = \"./dataset\"\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Dataset not found at {dataset_path} or ./dataset\")\n",
    "    \n",
    "    csv_files = sorted([f for f in os.listdir(dataset_path) if f.endswith('.csv')])\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {dataset_path}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\\n\")\n",
    "    \n",
    "    # ONLY load the daily historical data file (the one with date, price, volume)\n",
    "    daily_file = 'crypto_historical_365days.csv'\n",
    "    ordered_files = [daily_file] if daily_file in csv_files else []\n",
    "    \n",
    "    if not ordered_files:\n",
    "        raise FileNotFoundError(f\"Daily historical file '{daily_file}' not found in {dataset_path}\")\n",
    "    \n",
    "    for csv_file in ordered_files:\n",
    "        file_path = os.path.join(dataset_path, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"Loading: {csv_file}\")\n",
    "        print(f\"  Total records: {len(df)}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Extract cryptocurrencies by coin_name or symbol\n",
    "        if 'coin_name' in df.columns:\n",
    "            coin_names = sorted(df['coin_name'].unique())\n",
    "            print(f\"  [OK] Found {len(coin_names)} unique cryptocurrencies\")\n",
    "            \n",
    "            for coin_name in coin_names:\n",
    "                coin_df = df[df['coin_name'] == coin_name].copy()\n",
    "                crypto_key = coin_name.lower().replace(' ', '_').strip()\n",
    "                crypto_data[crypto_key] = coin_df\n",
    "                \n",
    "        elif 'symbol' in df.columns:\n",
    "            symbols = sorted(df['symbol'].unique())\n",
    "            print(f\"  [OK] Found {len(symbols)} unique cryptocurrencies\")\n",
    "            \n",
    "            for symbol in symbols:\n",
    "                symbol_df = df[df['symbol'] == symbol].copy()\n",
    "                crypto_key = symbol.lower().strip()\n",
    "                crypto_data[crypto_key] = symbol_df\n",
    "        else:\n",
    "            crypto_key = csv_file.replace('.csv', '').lower()\n",
    "            crypto_data[crypto_key] = df\n",
    "            print(f\"  [OK] Loaded as: {crypto_key}\")\n",
    "    \n",
    "    available_cryptos = sorted(list(crypto_data.keys()))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"AVAILABLE CRYPTOCURRENCIES ({len(available_cryptos)} total)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Display all cryptocurrencies with their record counts\n",
    "    for idx, crypto in enumerate(available_cryptos, 1):\n",
    "        display_name = crypto.replace('_', ' ').title()\n",
    "        record_count = len(crypto_data[crypto])\n",
    "        print(f\"{idx:3d}. {display_name:35s} ({record_count:7d} records)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    selection_input = input(\"Enter cryptocurrency number or name (e.g., 1 or bitcoin): \").strip()\n",
    "    \n",
    "    selected_crypto = None\n",
    "    \n",
    "    # Try numeric selection first\n",
    "    try:\n",
    "        idx = int(selection_input) - 1\n",
    "        if 0 <= idx < len(available_cryptos):\n",
    "            selected_crypto = available_cryptos[idx]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Try name matching\n",
    "    if not selected_crypto:\n",
    "        search_term = selection_input.lower()\n",
    "        # Exact match first\n",
    "        if search_term in crypto_data:\n",
    "            selected_crypto = search_term\n",
    "        else:\n",
    "            # Fuzzy match\n",
    "            matches = [c for c in available_cryptos if search_term in c]\n",
    "            if matches:\n",
    "                selected_crypto = matches[0]\n",
    "    \n",
    "    # Default fallback to first cryptocurrency\n",
    "    if not selected_crypto:\n",
    "        print(f\"[WARNING] '{selection_input}' not found. Using: {available_cryptos[0].title()}\")\n",
    "        selected_crypto = available_cryptos[0]\n",
    "    \n",
    "    display_name = selected_crypto.replace('_', ' ').title()\n",
    "    print(f\"\\n[OK] Selected: {display_name}\")\n",
    "    print(f\"  Records: {len(crypto_data[selected_crypto]):,}\")\n",
    "    print(f\"  Columns: {list(crypto_data[selected_crypto].columns)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[ERROR] Dataset Error: {e}\")\n",
    "    print(\"\\nEnsure the following:\")\n",
    "    print(\"1. Kaggle dataset has been downloaded (run the previous cell)\")\n",
    "    print(\"2. OR local ./dataset folder exists with CSV files\")\n",
    "    crypto_data = {}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error loading cryptocurrency data: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()\n",
    "    crypto_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65a37e",
   "metadata": {},
   "source": [
    "## Section 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section creates visualizations to understand cryptocurrency price movements and market behavior. It plots price trends for major cryptocurrencies alongside volume analysis, return distributions, and cumulative performance to reveal patterns and anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and set dataset path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_colab = True\n",
    "    dataset_path = \"/content/crypto_dataset\"\n",
    "    print(\"[OK] Running in Google Colab\")\n",
    "except ImportError:\n",
    "    is_colab = False\n",
    "    print(\"[OK] Running locally\")\n",
    "    # Try local dataset first\n",
    "    if os.path.exists(\"./dataset\"):\n",
    "        dataset_path = \"./dataset\"\n",
    "    else:\n",
    "        dataset_path = \"/content/crypto_dataset\"  # Fallback\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    if selected_crypto in crypto_data:\n",
    "        crypto_df = crypto_data[selected_crypto].copy()\n",
    "        crypto_df = crypto_df.sort_values('date') if 'date' in crypto_df.columns else crypto_df\n",
    "        \n",
    "        # Get the price column (could be 'price' or 'close')\n",
    "        price_col = 'price' if 'price' in crypto_df.columns else 'close' if 'close' in crypto_df.columns else None\n",
    "        volume_col = 'volume' if 'volume' in crypto_df.columns else None\n",
    "        \n",
    "        if volume_col and price_col:\n",
    "            # Plot 1: Volume over time\n",
    "            axes[0, 0].bar(range(len(crypto_df)), crypto_df[volume_col], color='#A23B72', alpha=0.7)\n",
    "            axes[0, 0].set_title(f\"{selected_crypto.title()} Trading Volume Over Time\", fontweight='bold')\n",
    "            axes[0, 0].set_xlabel(\"Time Period\")\n",
    "            axes[0, 0].set_ylabel(\"Volume\")\n",
    "            \n",
    "            # Plot 2: Returns distribution\n",
    "            crypto_df['returns'] = crypto_df[price_col].pct_change() * 100\n",
    "            axes[0, 1].hist(crypto_df['returns'].dropna(), bins=50, color='#F18F01', alpha=0.7, edgecolor='black')\n",
    "            axes[0, 1].set_title(f\"{selected_crypto.title()} Daily Returns Distribution\", fontweight='bold')\n",
    "            axes[0, 1].set_xlabel(\"Returns (%)\")\n",
    "            axes[0, 1].set_ylabel(\"Frequency\")\n",
    "            axes[0, 1].axvline(crypto_df['returns'].mean(), color='red', linestyle='--', \n",
    "                              label=f\"Mean: {crypto_df['returns'].mean():.2f}%\")\n",
    "            axes[0, 1].legend()\n",
    "            \n",
    "            # Plot 3: Cumulative returns\n",
    "            crypto_df['cumulative_returns'] = (1 + crypto_df['returns'] / 100).cumprod() - 1\n",
    "            axes[1, 0].plot(range(len(crypto_df)), crypto_df['cumulative_returns'] * 100, \n",
    "                          linewidth=2, color='#2E86AB')\n",
    "            axes[1, 0].set_title(f\"{selected_crypto.title()} Cumulative Returns\", fontweight='bold')\n",
    "            axes[1, 0].set_xlabel(\"Time Period\")\n",
    "            axes[1, 0].set_ylabel(\"Cumulative Returns (%)\")\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 4: Price distribution\n",
    "            axes[1, 1].hist(crypto_df[price_col].dropna(), bins=50, color='#06A77D', alpha=0.7, edgecolor='black')\n",
    "            axes[1, 1].set_title(f\"{selected_crypto.title()} Price Distribution\", fontweight='bold')\n",
    "            axes[1, 1].set_xlabel(\"Price (USD)\")\n",
    "            axes[1, 1].set_ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/02_eda_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"[OK] EDA analysis visualization saved\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error creating EDA analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c043e",
   "metadata": {},
   "source": [
    "## Section 5: Correlation Analysis\n",
    "\n",
    "This section calculates the price correlations between different cryptocurrencies to understand how they move together. It generates a heatmap visualization and analyzes whether altcoins follow Bitcoin's trends by examining correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e089705",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if selected_crypto not in crypto_data or len(crypto_data[selected_crypto]) == 0:\n",
    "        print(\"[ERROR] No data available for selected cryptocurrency\")\n",
    "    else:\n",
    "        df = crypto_data[selected_crypto].copy()\n",
    "        \n",
    "        print(f\"Analyzing Correlation Patterns for {selected_crypto.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Identify numeric columns for correlation analysis\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if len(numeric_cols) < 2:\n",
    "            print(f\"[WARNING] Insufficient numeric columns for correlation analysis\")\n",
    "            print(f\"Found columns: {numeric_cols}\")\n",
    "        else:\n",
    "            # Select relevant numeric columns\n",
    "            correlation_cols = [col for col in numeric_cols if col not in ['coin_id', 'market_cap_rank']]\n",
    "            \n",
    "            if len(correlation_cols) > 0:\n",
    "                corr_matrix = df[correlation_cols].corr()\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                            fmt='.2f', square=True, cbar_kws={'label': 'Correlation'})\n",
    "                plt.title(f'Correlation Matrix - {selected_crypto.upper()}', fontsize=14, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if save_plots:\n",
    "                    plot_name = f\"{selected_crypto}_correlation_matrix.png\"\n",
    "                    plot_path = os.path.join(output_dir, plot_name)\n",
    "                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"[OK] Correlation matrix saved to {plot_name}\")\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "                # Print top correlations\n",
    "                print(\"\\nTop Correlations (excluding self-correlation):\")\n",
    "                print(\"-\"*70)\n",
    "                \n",
    "                corr_pairs = []\n",
    "                for i in range(len(corr_matrix.columns)):\n",
    "                    for j in range(i+1, len(corr_matrix.columns)):\n",
    "                        corr_pairs.append({\n",
    "                            'var1': corr_matrix.columns[i],\n",
    "                            'var2': corr_matrix.columns[j],\n",
    "                            'correlation': corr_matrix.iloc[i, j]\n",
    "                        })\n",
    "                \n",
    "                corr_pairs.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "                \n",
    "                for idx, pair in enumerate(corr_pairs[:10], 1):\n",
    "                    corr_val = pair['correlation']\n",
    "                    strength = 'Strong' if abs(corr_val) > 0.7 else 'Moderate' if abs(corr_val) > 0.4 else 'Weak'\n",
    "                    print(f\"{idx}. {pair['var1']:20s} <-> {pair['var2']:20s}: {corr_val:7.3f} ({strength})\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            else:\n",
    "                print(\"[WARNING] No suitable numeric columns found for correlation analysis\")\n",
    "                print(f\"Available columns: {numeric_cols}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in correlation analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d99f8b",
   "metadata": {},
   "source": [
    "## Section 6: Moving Average Strategy Implementation\n",
    "\n",
    "This section implements and tests a moving average crossover trading strategy that generates buy and sell signals when short-term moving averages cross long-term moving averages. It compares the strategy performance against a simple buy-and-hold approach to evaluate strategy effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if selected_crypto not in crypto_data or len(crypto_data[selected_crypto]) == 0:\n",
    "        print(\"[ERROR] No data available for selected cryptocurrency\")\n",
    "    else:\n",
    "        df = crypto_data[selected_crypto].copy()\n",
    "        \n",
    "        print(f\"Analyzing Risk and Volatility Metrics for {selected_crypto.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Check for required columns\n",
    "        price_col = None\n",
    "        return_col = None\n",
    "        \n",
    "        if 'price' in df.columns:\n",
    "            price_col = 'price'\n",
    "        elif 'close' in df.columns:\n",
    "            price_col = 'close'\n",
    "        elif 'avg_price' in df.columns:\n",
    "            price_col = 'avg_price'\n",
    "        \n",
    "        if 'daily_return' in df.columns:\n",
    "            return_col = 'daily_return'\n",
    "        \n",
    "        if price_col is None:\n",
    "            print(f\"[WARNING] No price column found in data\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "        else:\n",
    "            # Calculate volatility metrics\n",
    "            df_sorted = df.sort_values('date') if 'date' in df.columns else df\n",
    "            \n",
    "            # Calculate returns if not already present\n",
    "            if return_col is None:\n",
    "                returns = df_sorted[price_col].pct_change().dropna()\n",
    "            else:\n",
    "                returns = df_sorted[return_col].dropna()\n",
    "            \n",
    "            if len(returns) == 0:\n",
    "                print(\"[WARNING] Unable to calculate returns from available data\")\n",
    "            else:\n",
    "                # Calculate volatility metrics\n",
    "                volatility = returns.std()\n",
    "                sharpe_ratio = (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0\n",
    "                max_drawdown = ((returns + 1).cumprod() - (returns + 1).cumprod().cummax()) / (returns + 1).cumprod().cummax()\n",
    "                max_dd = max_drawdown.min()\n",
    "                \n",
    "                print(f\"\\nVolatility Metrics:\")\n",
    "                print(\"-\"*70)\n",
    "                print(f\"Daily Return Mean:        {returns.mean():10.6f} ({returns.mean()*100:7.3f}%)\")\n",
    "                print(f\"Daily Volatility (Std):   {volatility:10.6f} ({volatility*100:7.3f}%)\")\n",
    "                print(f\"Annual Volatility:        {volatility*np.sqrt(252):10.6f} ({volatility*np.sqrt(252)*100:7.3f}%)\")\n",
    "                print(f\"Sharpe Ratio (Annual):    {sharpe_ratio:10.4f}\")\n",
    "                print(f\"Maximum Drawdown:         {max_dd:10.6f} ({max_dd*100:7.3f}%)\")\n",
    "                print(f\"Min Return:               {returns.min():10.6f} ({returns.min()*100:7.3f}%)\")\n",
    "                print(f\"Max Return:               {returns.max():10.6f} ({returns.max()*100:7.3f}%)\")\n",
    "                \n",
    "                # Visualize volatility\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "                \n",
    "                # Rolling volatility\n",
    "                rolling_vol = returns.rolling(window=30).std()\n",
    "                axes[0, 0].plot(rolling_vol, color='darkred', linewidth=1.5)\n",
    "                axes[0, 0].set_title('30-Day Rolling Volatility', fontweight='bold')\n",
    "                axes[0, 0].set_ylabel('Volatility')\n",
    "                axes[0, 0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Returns distribution\n",
    "                axes[0, 1].hist(returns, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "                axes[0, 1].set_title('Distribution of Daily Returns', fontweight='bold')\n",
    "                axes[0, 1].set_xlabel('Return')\n",
    "                axes[0, 1].set_ylabel('Frequency')\n",
    "                axes[0, 1].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Cumulative returns\n",
    "                cumulative_returns = (returns + 1).cumprod()\n",
    "                axes[1, 0].plot(cumulative_returns.index, cumulative_returns.values, color='darkgreen', linewidth=2)\n",
    "                axes[1, 0].set_title('Cumulative Returns', fontweight='bold')\n",
    "                axes[1, 0].set_ylabel('Cumulative Multiplier')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Drawdown\n",
    "                drawdown = (cumulative_returns - cumulative_returns.cummax()) / cumulative_returns.cummax()\n",
    "                axes[1, 1].fill_between(drawdown.index, drawdown.values, 0, color='darkred', alpha=0.5)\n",
    "                axes[1, 1].set_title('Drawdown Over Time', fontweight='bold')\n",
    "                axes[1, 1].set_ylabel('Drawdown %')\n",
    "                axes[1, 1].grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if save_plots:\n",
    "                    plot_name = f\"{selected_crypto}_volatility_analysis.png\"\n",
    "                    plot_path = os.path.join(output_dir, plot_name)\n",
    "                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"\\n[OK] Volatility analysis saved to {plot_name}\")\n",
    "                \n",
    "                plt.show()\n",
    "                print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in volatility analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c88bea",
   "metadata": {},
   "source": [
    "## Section 7: Volatility Analysis\n",
    "\n",
    "This section calculates volatility metrics and risk-adjusted returns for each cryptocurrency. It computes 30-day and 60-day annualized volatility, Sharpe ratios, and maximum drawdowns to identify which assets offer the best risk-adjusted performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a91712",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if selected_crypto not in crypto_data or len(crypto_data[selected_crypto]) == 0:\n",
    "        print(\"[ERROR] No data available for selected cryptocurrency\")\n",
    "    else:\n",
    "        df = crypto_data[selected_crypto].copy()\n",
    "        \n",
    "        print(f\"Analyzing Seasonality Patterns for {selected_crypto.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Check for date and return columns\n",
    "        date_col = None\n",
    "        if 'date' in df.columns:\n",
    "            date_col = 'date'\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        elif 'timestamp' in df.columns:\n",
    "            date_col = 'timestamp'\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        if date_col is None:\n",
    "            print(\"[WARNING] No date column found. Using index as time series.\")\n",
    "            df['date'] = pd.date_range(start='2024-01-01', periods=len(df), freq='D')\n",
    "            date_col = 'date'\n",
    "        \n",
    "        # Find return column\n",
    "        return_col = None\n",
    "        if 'daily_return' in df.columns:\n",
    "            return_col = 'daily_return'\n",
    "        elif 'price' in df.columns:\n",
    "            return_col = 'price'\n",
    "            df['returns'] = df['price'].pct_change()\n",
    "            return_col = 'returns'\n",
    "        \n",
    "        if return_col is None or df[return_col].isna().all():\n",
    "            print(\"[WARNING] Unable to find valid return column\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "        else:\n",
    "            df_sorted = df.sort_values(date_col).reset_index(drop=True)\n",
    "            df_sorted['date'] = pd.to_datetime(df_sorted[date_col])\n",
    "            \n",
    "            # Extract time features\n",
    "            df_sorted['year'] = df_sorted['date'].dt.year\n",
    "            df_sorted['month'] = df_sorted['date'].dt.month\n",
    "            df_sorted['day_of_week'] = df_sorted['date'].dt.dayofweek\n",
    "            df_sorted['week'] = df_sorted['date'].dt.isocalendar().week\n",
    "            \n",
    "            # Monthly seasonality\n",
    "            monthly_returns = df_sorted.groupby('month')[return_col].mean() * 100\n",
    "            \n",
    "            # Day of week seasonality\n",
    "            day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            dow_returns = df_sorted.groupby('day_of_week')[return_col].mean() * 100\n",
    "            \n",
    "            # Visualize seasonality\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Monthly seasonality\n",
    "            colors_m = ['green' if x > 0 else 'red' for x in monthly_returns.values]\n",
    "            axes[0].bar(monthly_returns.index, monthly_returns.values, color=colors_m, edgecolor='black', alpha=0.7)\n",
    "            axes[0].set_title('Average Return by Month', fontweight='bold')\n",
    "            axes[0].set_xlabel('Month')\n",
    "            axes[0].set_ylabel('Average Return (%)')\n",
    "            axes[0].set_xticks(range(1, 13))\n",
    "            axes[0].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "            axes[0].grid(True, alpha=0.3, axis='y')\n",
    "            axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "            \n",
    "            # Day of week seasonality\n",
    "            colors_d = ['green' if x > 0 else 'red' for x in dow_returns.values]\n",
    "            axes[1].bar(range(len(day_names)), dow_returns.values, color=colors_d, edgecolor='black', alpha=0.7)\n",
    "            axes[1].set_title('Average Return by Day of Week', fontweight='bold')\n",
    "            axes[1].set_xlabel('Day of Week')\n",
    "            axes[1].set_ylabel('Average Return (%)')\n",
    "            axes[1].set_xticks(range(len(day_names)))\n",
    "            axes[1].set_xticklabels(day_names, rotation=45)\n",
    "            axes[1].grid(True, alpha=0.3, axis='y')\n",
    "            axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_plots:\n",
    "                plot_name = f\"{selected_crypto}_seasonality_analysis.png\"\n",
    "                plot_path = os.path.join(output_dir, plot_name)\n",
    "                plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"[OK] Seasonality analysis saved to {plot_name}\")\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nMonthly Seasonality (Average Return %):\")\n",
    "            print(\"-\"*70)\n",
    "            for month, ret in monthly_returns.items():\n",
    "                month_name = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "                              'July', 'August', 'September', 'October', 'November', 'December'][month-1]\n",
    "                trend = \"bullish\" if ret > 0 else \"bearish\"\n",
    "                print(f\"{month_name:12s}: {ret:8.3f}% ({trend})\")\n",
    "            \n",
    "            print(f\"\\nDay of Week Seasonality (Average Return %):\")\n",
    "            print(\"-\"*70)\n",
    "            for dow, ret in dow_returns.items():\n",
    "                trend = \"bullish\" if ret > 0 else \"bearish\"\n",
    "                print(f\"{day_names[dow]:12s}: {ret:8.3f}% ({trend})\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in seasonality analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0100c",
   "metadata": {},
   "source": [
    "## Section 8: Seasonal Pattern Detection\n",
    "\n",
    "This section analyzes temporal patterns in cryptocurrency prices by aggregating returns by month, quarter, and day of week. It identifies recurring trends that can inform trading strategies and helps understand whether certain periods consistently outperform others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if selected_crypto not in crypto_data or len(crypto_data[selected_crypto]) == 0:\n",
    "        print(\"[ERROR] No data available for selected cryptocurrency\")\n",
    "    else:\n",
    "        df = crypto_data[selected_crypto].copy()\n",
    "        \n",
    "        print(f\"Portfolio Optimization Analysis for {selected_crypto.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Find price column\n",
    "        price_col = None\n",
    "        if 'price' in df.columns:\n",
    "            price_col = 'price'\n",
    "        elif 'close' in df.columns:\n",
    "            price_col = 'close'\n",
    "        elif 'avg_price' in df.columns:\n",
    "            price_col = 'avg_price'\n",
    "        elif 'start_price' in df.columns:\n",
    "            price_col = 'start_price'\n",
    "        \n",
    "        if price_col is None:\n",
    "            print(f\"[WARNING] No price column found\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "        else:\n",
    "            # Sort by date if available\n",
    "            if 'date' in df.columns:\n",
    "                df_sorted = df.sort_values('date').reset_index(drop=True)\n",
    "            elif 'timestamp' in df.columns:\n",
    "                df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
    "            else:\n",
    "                df_sorted = df.reset_index(drop=True)\n",
    "            \n",
    "            prices = df_sorted[price_col].dropna()\n",
    "            \n",
    "            if len(prices) < 2:\n",
    "                print(\"[ERROR] Insufficient price data for portfolio analysis\")\n",
    "            else:\n",
    "                # Calculate returns\n",
    "                returns = prices.pct_change().dropna()\n",
    "                \n",
    "                if len(returns) == 0 or returns.std() == 0:\n",
    "                    print(\"[WARNING] Unable to calculate valid returns from price data\")\n",
    "                else:\n",
    "                    # Calculate optimal weights\n",
    "                    exp_returns = returns.mean() * 252\n",
    "                    volatility = returns.std() * np.sqrt(252)\n",
    "                    \n",
    "                    if volatility == 0:\n",
    "                        print(\"[WARNING] Zero volatility - unable to calculate Sharpe ratio\")\n",
    "                        sharpe_ratio = 0\n",
    "                    else:\n",
    "                        sharpe_ratio = exp_returns / volatility\n",
    "                    \n",
    "                    print(f\"\\nPortfolio Metrics:\")\n",
    "                    print(\"-\"*70)\n",
    "                    print(f\"Expected Annual Return: {exp_returns*100:8.2f}%\")\n",
    "                    print(f\"Annual Volatility:      {volatility*100:8.2f}%\")\n",
    "                    print(f\"Sharpe Ratio:           {sharpe_ratio:8.4f}\")\n",
    "                    \n",
    "                    # Create allocation visualization\n",
    "                    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "                    \n",
    "                    # Single asset allocation (for single crypto)\n",
    "                    allocation = [100]\n",
    "                    labels = [selected_crypto.upper()]\n",
    "                    colors_pie = plt.cm.Set3(np.linspace(0, 1, len(labels)))\n",
    "                    \n",
    "                    axes[0].pie(allocation, labels=labels, autopct='%1.1f%%', \n",
    "                               colors=colors_pie, startangle=90, textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "                    axes[0].set_title(f'Portfolio Allocation - {selected_crypto.upper()}', fontweight='bold')\n",
    "                    \n",
    "                    # Efficient frontier (simulated for single asset)\n",
    "                    risk_levels = np.linspace(0, volatility*2, 100)\n",
    "                    returns_levels = exp_returns * (risk_levels / volatility) if volatility > 0 else risk_levels * 0\n",
    "                    \n",
    "                    axes[1].scatter([volatility], [exp_returns], s=200, color='red', marker='*', \n",
    "                                   label='Current Allocation', zorder=5, edgecolor='black', linewidth=2)\n",
    "                    axes[1].plot(risk_levels, returns_levels, 'b--', linewidth=2, label='Efficient Frontier')\n",
    "                    axes[1].scatter([0], [0], s=100, color='green', marker='o', label='Risk-Free Rate')\n",
    "                    \n",
    "                    axes[1].set_xlabel('Annual Volatility (Risk)', fontweight='bold')\n",
    "                    axes[1].set_ylabel('Expected Annual Return', fontweight='bold')\n",
    "                    axes[1].set_title('Efficient Frontier Analysis', fontweight='bold')\n",
    "                    axes[1].legend(loc='upper left')\n",
    "                    axes[1].grid(True, alpha=0.3)\n",
    "                    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    if save_plots:\n",
    "                        plot_name = f\"{selected_crypto}_portfolio_optimization.png\"\n",
    "                        plot_path = os.path.join(output_dir, plot_name)\n",
    "                        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                        print(f\"\\n[OK] Portfolio analysis saved to {plot_name}\")\n",
    "                    \n",
    "                    plt.show()\n",
    "                    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in portfolio optimization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e00dbf",
   "metadata": {},
   "source": [
    "## Section 9: Portfolio Optimization\n",
    "\n",
    "This section applies Modern Portfolio Theory to find the optimal asset allocation that maximizes risk-adjusted returns. It uses Monte Carlo simulation to generate the efficient frontier and identifies portfolios with the highest Sharpe ratios and minimum volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if selected_crypto not in crypto_data or len(crypto_data[selected_crypto]) == 0:\n",
    "        print(\"[ERROR] No data available for selected cryptocurrency\")\n",
    "    else:\n",
    "        df = crypto_data[selected_crypto].copy()\n",
    "        \n",
    "        print(f\"=\"*70)\n",
    "        print(f\"PREPARING DATA FOR MACHINE LEARNING - {selected_crypto.upper()}\")\n",
    "        print(f\"=\"*70)\n",
    "        \n",
    "        # Find price and date columns\n",
    "        price_col = None\n",
    "        date_col = None\n",
    "        volume_col = None\n",
    "        \n",
    "        if 'price' in df.columns:\n",
    "            price_col = 'price'\n",
    "        elif 'close' in df.columns:\n",
    "            price_col = 'close'\n",
    "        elif 'avg_price' in df.columns:\n",
    "            price_col = 'avg_price'\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            date_col = 'date'\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        elif 'timestamp' in df.columns:\n",
    "            date_col = 'timestamp'\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        if 'volume' in df.columns:\n",
    "            volume_col = 'volume'\n",
    "        elif 'total_volume' in df.columns:\n",
    "            volume_col = 'total_volume'\n",
    "        \n",
    "        # Sort by date if available\n",
    "        if date_col:\n",
    "            df = df.sort_values(date_col).reset_index(drop=True)\n",
    "        \n",
    "        if price_col is None:\n",
    "            print(f\"[ERROR] No price column found for ML analysis\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "        elif len(df) < 100:\n",
    "            print(f\"[ERROR] Insufficient data points ({len(df)}) for training ML models\")\n",
    "        else:\n",
    "            # Create feature engineering\n",
    "            df['returns'] = df[price_col].pct_change()\n",
    "            \n",
    "            # Moving averages\n",
    "            df['ma_7'] = df[price_col].rolling(window=7, min_periods=1).mean()\n",
    "            df['ma_30'] = df[price_col].rolling(window=30, min_periods=1).mean()\n",
    "            \n",
    "            # Volatility\n",
    "            df['volatility'] = df['returns'].rolling(window=30, min_periods=1).std()\n",
    "            \n",
    "            # Volume features if available\n",
    "            if volume_col:\n",
    "                df['volume_ma'] = df[volume_col].rolling(window=7, min_periods=1).mean()\n",
    "                df['volume_ratio'] = df[volume_col] / df['volume_ma'] if (df['volume_ma'] != 0).all() else 1.0\n",
    "            else:\n",
    "                df['volume_ma'] = 0\n",
    "                df['volume_ratio'] = 1.0\n",
    "            \n",
    "            # High-low ratio if available\n",
    "            if 'high' in df.columns and 'low' in df.columns:\n",
    "                df['high_low_ratio'] = (df['high'] - df['low']) / df[price_col]\n",
    "            else:\n",
    "                df['high_low_ratio'] = 0\n",
    "            \n",
    "            # Remove rows with NaN from feature engineering\n",
    "            df = df.dropna(subset=['returns', 'ma_7', 'ma_30', 'volatility'])\n",
    "            \n",
    "            print(f\"\\n[OK] Data loaded and engineered\")\n",
    "            print(f\"  Total records: {len(df):,}\")\n",
    "            print(f\"  Price column: {price_col}\")\n",
    "            print(f\"  Date range: {df[date_col].min() if date_col else 'N/A'} to {df[date_col].max() if date_col else 'N/A'}\")\n",
    "            \n",
    "            # Normalize the price data\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            \n",
    "            scaler_price = MinMaxScaler(feature_range=(0, 1))\n",
    "            df['price_scaled'] = scaler_price.fit_transform(df[[price_col]])\n",
    "            \n",
    "            # Train-test split\n",
    "            train_size = int(len(df) * 0.8)\n",
    "            train_data = df.iloc[:train_size].copy()\n",
    "            test_data = df.iloc[train_size:].copy()\n",
    "            \n",
    "            print(f\"\\n[OK] Train-test split complete\")\n",
    "            print(f\"  Training samples: {len(train_data):,}\")\n",
    "            print(f\"  Testing samples: {len(test_data):,}\")\n",
    "            if date_col:\n",
    "                print(f\"  Training period: {train_data[date_col].min()} to {train_data[date_col].max()}\")\n",
    "                print(f\"  Testing period: {test_data[date_col].min()} to {test_data[date_col].max()}\")\n",
    "            \n",
    "            # Create sequences for deep learning\n",
    "            def create_sequences(data, seq_length=60):\n",
    "                X, y = [], []\n",
    "                for i in range(len(data) - seq_length):\n",
    "                    X.append(data[i:i+seq_length])\n",
    "                    y.append(data[i+seq_length])\n",
    "                return np.array(X), np.array(y)\n",
    "            \n",
    "            # Prepare sequences\n",
    "            train_sequences = train_data['price_scaled'].values\n",
    "            test_sequences = test_data['price_scaled'].values\n",
    "            \n",
    "            seq_length = 60\n",
    "            X_train_lstm, y_train_lstm = create_sequences(train_sequences, seq_length)\n",
    "            X_test_lstm, y_test_lstm = create_sequences(test_sequences, seq_length)\n",
    "            \n",
    "            print(f\"\\n[OK] LSTM/GRU sequences created\")\n",
    "            print(f\"  Sequence length: {seq_length}\")\n",
    "            print(f\"  Training X shape: {X_train_lstm.shape}\")\n",
    "            print(f\"  Training y shape: {y_train_lstm.shape}\")\n",
    "            print(f\"  Testing X shape: {X_test_lstm.shape}\")\n",
    "            print(f\"  Testing y shape: {y_test_lstm.shape}\")\n",
    "            \n",
    "            print(f\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in data preparation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    df = None\n",
    "    X_train_lstm = None\n",
    "    y_train_lstm = None\n",
    "    X_test_lstm = None\n",
    "    y_test_lstm = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f8e2c",
   "metadata": {},
   "source": [
    "## Section 10: Data Preparation for Machine Learning Models\n",
    "\n",
    "This section prepares Bitcoin data for machine learning by engineering features such as moving averages and volatility indicators. It normalizes the data, creates sequential samples for deep learning models, and splits the dataset into training and testing sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has been merged with the updated data preparation in the previous cell.\n",
    "# All ML data is now prepared in the previous section.\n",
    "\n",
    "if 'df' not in locals():\n",
    "    print(\"[ERROR] Data preparation not completed successfully\")\n",
    "    print(\"Please run the data preparation cell first.\")\n",
    "else:\n",
    "    print(f\"\\n[OK] Data ready for ML model training\")\n",
    "    print(f\"  Dataset: {selected_crypto.upper()}\")\n",
    "    print(f\"  Total records: {len(df):,}\")\n",
    "    print(f\"  Training features prepared: returns, moving averages, volatility\")\n",
    "    print(f\"  Scaled prices prepared for LSTM/GRU models\")\n",
    "    print(\"\\nReady to proceed with ARIMA, XGBoost, and deep learning models\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877cc67f",
   "metadata": {},
   "source": [
    "## Section 11: ARIMA Model Implementation\n",
    "\n",
    "This section trains an AutoRegressive Integrated Moving Average model for time series forecasting. It first tests for stationarity, determines optimal parameters through differencing, and then generates price predictions using the fitted ARIMA model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"ARIMA MODEL TRAINING - {selected_crypto.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if 'df' not in locals() or df is None:\n",
    "        print(\"[ERROR] Data not prepared. Run data preparation cell first.\")\n",
    "    elif df.empty:\n",
    "        print(\"[ERROR] DataFrame is empty\")\n",
    "    else:\n",
    "        # Find price column\n",
    "        price_col = None\n",
    "        if 'price' in df.columns:\n",
    "            price_col = 'price'\n",
    "        elif 'close' in df.columns:\n",
    "            price_col = 'close'\n",
    "        elif 'avg_price' in df.columns:\n",
    "            price_col = 'avg_price'\n",
    "        \n",
    "        if price_col is None:\n",
    "            print(f\"[WARNING] No price column found\")\n",
    "        else:\n",
    "            # Get price data\n",
    "            price_data = df[price_col].dropna()\n",
    "            \n",
    "            if len(price_data) < 30:\n",
    "                print(f\"[ERROR] Insufficient data for ARIMA ({len(price_data)} records)\")\n",
    "            else:\n",
    "                print(f\"\\n[OK] Using {len(price_data)} price observations\\n\")\n",
    "                \n",
    "                # Test for stationarity\n",
    "                print(\"Stationarity Test (Augmented Dickey-Fuller):\")\n",
    "                print(\"-\"*70)\n",
    "                \n",
    "                adf_result = adfuller(price_data, autolag='AIC')\n",
    "                print(f\"ADF Statistic:      {adf_result[0]:.6f}\")\n",
    "                print(f\"P-Value:            {adf_result[1]:.6f}\")\n",
    "                print(f\"Critical Values:\")\n",
    "                for key, value in adf_result[4].items():\n",
    "                    print(f\"  {key:3s}: {value:.3f}\")\n",
    "                \n",
    "                if adf_result[1] < 0.05:\n",
    "                    print(\"\\n[OK] Series is stationary (p < 0.05)\")\n",
    "                    d_value = 0\n",
    "                else:\n",
    "                    print(\"\\n[WARNING] Series is non-stationary. Using differencing (d=1)\")\n",
    "                    d_value = 1\n",
    "                \n",
    "                # Fit ARIMA model\n",
    "                print(f\"\\nFitting ARIMA({1},{d_value},{1}) model...\")\n",
    "                \n",
    "                arima_model = ARIMA(price_data, order=(1, d_value, 1))\n",
    "                arima_fitted = arima_model.fit()\n",
    "                \n",
    "                print(f\"\\n[OK] ARIMA model fitted successfully\")\n",
    "                print(f\"\\nModel Summary (top results):\")\n",
    "                print(\"-\"*70)\n",
    "                \n",
    "                summary_lines = str(arima_fitted.summary()).split('\\n')\n",
    "                for line in summary_lines[:20]:\n",
    "                    print(line)\n",
    "                \n",
    "                # Make predictions on test set\n",
    "                train_size = int(len(price_data) * 0.8)\n",
    "                test_size = len(price_data) - train_size\n",
    "                \n",
    "                predictions = arima_fitted.get_forecast(steps=test_size)\n",
    "                forecast = predictions.predicted_mean\n",
    "                forecast_ci = predictions.conf_int()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "                \n",
    "                test_actual = price_data.iloc[train_size:]\n",
    "                mae_arima = mean_absolute_error(test_actual, forecast)\n",
    "                rmse_arima = np.sqrt(mean_squared_error(test_actual, forecast))\n",
    "                mape_arima = np.mean(np.abs((test_actual.values - forecast.values) / test_actual.values)) * 100\n",
    "                \n",
    "                print(f\"\\n[OK] Predictions generated on test set ({test_size} samples)\")\n",
    "                print(f\"\\nModel Performance:\")\n",
    "                print(\"-\"*70)\n",
    "                print(f\"Mean Absolute Error (MAE):    ${mae_arima:.4f}\")\n",
    "                print(f\"Root Mean Squared Error (RMSE): ${rmse_arima:.4f}\")\n",
    "                print(f\"Mean Absolute Percentage Error: {mape_arima:.2f}%\")\n",
    "                \n",
    "                # Visualize results\n",
    "                fig, ax = plt.subplots(figsize=(14, 6))\n",
    "                \n",
    "                ax.plot(price_data.index[:train_size], price_data.iloc[:train_size], label='Training Data', linewidth=2)\n",
    "                ax.plot(price_data.index[train_size:], price_data.iloc[train_size:], label='Actual Test Data', linewidth=2)\n",
    "                ax.plot(forecast.index, forecast, label='ARIMA Forecast', linewidth=2, linestyle='--')\n",
    "                ax.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2, color='orange')\n",
    "                \n",
    "                ax.set_title(f'ARIMA Model Forecast - {selected_crypto.upper()}', fontweight='bold', fontsize=14)\n",
    "                ax.set_xlabel('Time')\n",
    "                ax.set_ylabel('Price')\n",
    "                ax.legend(loc='best')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if save_plots:\n",
    "                    plot_name = f\"{selected_crypto}_arima_forecast.png\"\n",
    "                    plot_path = os.path.join(output_dir, plot_name)\n",
    "                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"\\n[OK] ARIMA forecast saved to {plot_name}\")\n",
    "                \n",
    "                plt.show()\n",
    "                print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in ARIMA modeling: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ce9af",
   "metadata": {},
   "source": [
    "## Section 12: XGBoost Model Implementation\n",
    "\n",
    "This section trains an eXtreme Gradient Boosting model that learns from engineered technical features. It creates features based on returns, moving averages, and volatility, then trains the model to predict Bitcoin prices and identifies the most important features for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"XGBOOST MODEL TRAINING - {selected_crypto.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if 'df' not in locals() or df is None or df.empty:\n",
    "        print(\"[ERROR] Data not prepared. Run data preparation cell first.\")\n",
    "    elif 'train_data' not in locals() or 'test_data' not in locals():\n",
    "        print(\"[ERROR] Train/test data not available.\")\n",
    "    else:\n",
    "        print(\"\\n[OK] Building XGBoost model with engineered features...\\n\")\n",
    "        \n",
    "        # Select feature columns\n",
    "        feature_cols = ['returns', 'ma_7', 'ma_30', 'volatility', 'volume_ratio', 'high_low_ratio']\n",
    "        available_features = [col for col in feature_cols if col in train_data.columns]\n",
    "        \n",
    "        if len(available_features) == 0:\n",
    "            print(\"[ERROR] No feature columns found\")\n",
    "        else:\n",
    "            print(f\"Features: {available_features}\\n\")\n",
    "            \n",
    "            # Find price column\n",
    "            price_col = None\n",
    "            if 'price' in df.columns:\n",
    "                price_col = 'price'\n",
    "            elif 'close' in df.columns:\n",
    "                price_col = 'close'\n",
    "            elif 'avg_price' in df.columns:\n",
    "                price_col = 'avg_price'\n",
    "            \n",
    "            if price_col is None:\n",
    "                print(\"[ERROR] No price column for target\")\n",
    "            else:\n",
    "                # Prepare training data\n",
    "                X_train = train_data[available_features].fillna(0).values\n",
    "                y_train = train_data[price_col].values\n",
    "                \n",
    "                X_test = test_data[available_features].fillna(0).values\n",
    "                y_test = test_data[price_col].values\n",
    "                \n",
    "                # Normalize features\n",
    "                scaler_features = MinMaxScaler()\n",
    "                X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "                X_test_scaled = scaler_features.transform(X_test)\n",
    "                \n",
    "                print(f\"Training set: {X_train_scaled.shape[0]} samples, {X_train_scaled.shape[1]} features\")\n",
    "                print(f\"Testing set: {X_test_scaled.shape[0]} samples\\n\")\n",
    "                \n",
    "                # Train XGBoost model\n",
    "                print(\"Training XGBoost regressor...\")\n",
    "                xgb_model = xgb.XGBRegressor(\n",
    "                    n_estimators=200,\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=5,\n",
    "                    min_child_weight=1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42,\n",
    "                    verbosity=0\n",
    "                )\n",
    "                \n",
    "                xgb_model.fit(X_train_scaled, y_train, verbose=False)\n",
    "                \n",
    "                print(\"[OK] Model trained successfully\\n\")\n",
    "                \n",
    "                # Make predictions\n",
    "                xgb_train_pred = xgb_model.predict(X_train_scaled)\n",
    "                xgb_test_pred = xgb_model.predict(X_test_scaled)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                xgb_train_mae = mean_absolute_error(y_train, xgb_train_pred)\n",
    "                xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_train_pred))\n",
    "                \n",
    "                xgb_test_mae = mean_absolute_error(y_test, xgb_test_pred)\n",
    "                xgb_test_rmse = np.sqrt(mean_squared_error(y_test, xgb_test_pred))\n",
    "                \n",
    "                if np.min(np.abs(y_test)) > 0:\n",
    "                    xgb_test_mape = np.mean(np.abs((y_test - xgb_test_pred) / y_test)) * 100\n",
    "                else:\n",
    "                    xgb_test_mape = np.nan\n",
    "                \n",
    "                print(f\"[OK] XGBoost Model Performance:\")\n",
    "                print(\"-\"*70)\n",
    "                print(f\"Training   - MAE: ${xgb_train_mae:.4f}, RMSE: ${xgb_train_rmse:.4f}\")\n",
    "                print(f\"Testing    - MAE: ${xgb_test_mae:.4f}, RMSE: ${xgb_test_rmse:.4f}\")\n",
    "                if not np.isnan(xgb_test_mape):\n",
    "                    print(f\"Testing    - MAPE: {xgb_test_mape:.2f}%\")\n",
    "                \n",
    "                # Feature importance\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': available_features,\n",
    "                    'Importance': xgb_model.feature_importances_\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                print(f\"\\nTop 5 Important Features:\")\n",
    "                print(\"-\"*70)\n",
    "                for idx, row in feature_importance.head(5).iterrows():\n",
    "                    print(f\"{row['Feature']:20s}: {row['Importance']:.4f}\")\n",
    "                \n",
    "                # Visualize predictions\n",
    "                fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "                \n",
    "                # Training performance\n",
    "                axes[0].plot(y_train, label='Actual Train', linewidth=1.5, alpha=0.7)\n",
    "                axes[0].plot(xgb_train_pred, label='XGBoost Train Prediction', linewidth=1.5, alpha=0.7, linestyle='--')\n",
    "                axes[0].set_title(f'XGBoost - Training Set Predictions', fontweight='bold')\n",
    "                axes[0].set_ylabel('Price')\n",
    "                axes[0].legend(loc='best')\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Testing performance\n",
    "                axes[1].plot(y_test, label='Actual Test', linewidth=1.5, alpha=0.7)\n",
    "                axes[1].plot(xgb_test_pred, label='XGBoost Test Prediction', linewidth=1.5, alpha=0.7, linestyle='--')\n",
    "                axes[1].set_title(f'XGBoost - Test Set Predictions', fontweight='bold')\n",
    "                axes[1].set_xlabel('Sample')\n",
    "                axes[1].set_ylabel('Price')\n",
    "                axes[1].legend(loc='best')\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if save_plots:\n",
    "                    plot_name = f\"{selected_crypto}_xgboost_predictions.png\"\n",
    "                    plot_path = os.path.join(output_dir, plot_name)\n",
    "                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"\\n[OK] XGBoost predictions saved to {plot_name}\")\n",
    "                \n",
    "                plt.show()\n",
    "                print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in XGBoost modeling: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6454c4a",
   "metadata": {},
   "source": [
    "## Section 13: LSTM Model Implementation\n",
    "\n",
    "This section builds a Long Short-Term Memory neural network that processes sequential price data to capture temporal dependencies. The model stacks multiple LSTM layers with dropout regularization to prevent overfitting and predicts future Bitcoin prices based on historical sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"LSTM MODEL TRAINING - {selected_crypto.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if 'X_train_lstm' not in locals() or X_train_lstm is None:\n",
    "        print(\"[ERROR] LSTM sequences not prepared. Run data preparation cell first.\")\n",
    "    elif len(X_train_lstm) == 0:\n",
    "        print(\"[ERROR] Training data is empty\")\n",
    "    else:\n",
    "        print(f\"\\n[OK] Building LSTM model...\")\n",
    "        print(f\"  Input shape: {X_train_lstm.shape}\")\n",
    "        print(f\"  Training samples: {len(X_train_lstm)}\")\n",
    "        print(f\"  Testing samples: {len(X_test_lstm)}\\n\")\n",
    "        \n",
    "        # Build LSTM model\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(units=50, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(units=25),\n",
    "            Dropout(0.2),\n",
    "            Dense(units=1)\n",
    "        ])\n",
    "        \n",
    "        lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "        \n",
    "        print(\"Training LSTM model...\")\n",
    "        history_lstm = lstm_model.fit(\n",
    "            X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1)),\n",
    "            y_train_lstm,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        print(\"[OK] LSTM training complete\\n\")\n",
    "        \n",
    "        # Make predictions\n",
    "        lstm_train_pred_scaled = lstm_model.predict(\n",
    "            X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1)),\n",
    "            verbose=0\n",
    "        )\n",
    "        lstm_test_pred_scaled = lstm_model.predict(\n",
    "            X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1)),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Inverse scale predictions\n",
    "        lstm_train_pred = scaler_price.inverse_transform(lstm_train_pred_scaled)\n",
    "        lstm_test_pred = scaler_price.inverse_transform(lstm_test_pred_scaled)\n",
    "        \n",
    "        # Inverse scale actual values\n",
    "        y_train_actual = scaler_price.inverse_transform(y_train_lstm.reshape(-1, 1))\n",
    "        y_test_actual = scaler_price.inverse_transform(y_test_lstm.reshape(-1, 1))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "        \n",
    "        lstm_train_mae = mean_absolute_error(y_train_actual, lstm_train_pred)\n",
    "        lstm_train_rmse = np.sqrt(mean_squared_error(y_train_actual, lstm_train_pred))\n",
    "        \n",
    "        lstm_test_mae = mean_absolute_error(y_test_actual, lstm_test_pred)\n",
    "        lstm_test_rmse = np.sqrt(mean_squared_error(y_test_actual, lstm_test_pred))\n",
    "        \n",
    "        if np.min(np.abs(y_test_actual)) > 0:\n",
    "            lstm_test_mape = np.mean(np.abs((y_test_actual - lstm_test_pred) / y_test_actual)) * 100\n",
    "        else:\n",
    "            lstm_test_mape = np.nan\n",
    "        \n",
    "        print(f\"[OK] LSTM Model Performance:\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"Training   - MAE: ${lstm_train_mae:.4f}, RMSE: ${lstm_train_rmse:.4f}\")\n",
    "        print(f\"Testing    - MAE: ${lstm_test_mae:.4f}, RMSE: ${lstm_test_rmse:.4f}\")\n",
    "        if not np.isnan(lstm_test_mape):\n",
    "            print(f\"Testing    - MAPE: {lstm_test_mape:.2f}%\")\n",
    "        \n",
    "        # Visualize training history and predictions\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Training history\n",
    "        axes[0, 0].plot(history_lstm.history['loss'], label='Training Loss', linewidth=2)\n",
    "        axes[0, 0].plot(history_lstm.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "        axes[0, 0].set_title('LSTM - Training History', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training predictions\n",
    "        axes[0, 1].plot(y_train_actual, label='Actual Train', linewidth=1.5, alpha=0.7)\n",
    "        axes[0, 1].plot(lstm_train_pred, label='LSTM Train Prediction', linewidth=1.5, alpha=0.7, linestyle='--')\n",
    "        axes[0, 1].set_title('LSTM - Training Set', fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Price')\n",
    "        axes[0, 1].legend(loc='best')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Testing predictions\n",
    "        axes[1, 0].plot(y_test_actual, label='Actual Test', linewidth=1.5, alpha=0.7)\n",
    "        axes[1, 0].plot(lstm_test_pred, label='LSTM Test Prediction', linewidth=1.5, alpha=0.7, linestyle='--')\n",
    "        axes[1, 0].set_title('LSTM - Test Set', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Sample')\n",
    "        axes[1, 0].set_ylabel('Price')\n",
    "        axes[1, 0].legend(loc='best')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = y_test_actual - lstm_test_pred\n",
    "        axes[1, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        axes[1, 1].set_title('LSTM - Test Residuals Distribution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Residual')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            plot_name = f\"{selected_crypto}_lstm_predictions.png\"\n",
    "            plot_path = os.path.join(output_dir, plot_name)\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\n[OK] LSTM predictions saved to {plot_name}\")\n",
    "        \n",
    "        plt.show()\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in LSTM modeling: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7c91a",
   "metadata": {},
   "source": [
    "## Section 14: GRU Model Implementation\n",
    "\n",
    "This section constructs a Gated Recurrent Unit neural network with a similar architecture to LSTM but with fewer parameters. The GRU model processes sequential price data using gating mechanisms to selectively retain relevant information across time steps for predicting Bitcoin prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"GRU MODEL TRAINING - {selected_crypto.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if 'X_train_lstm' not in locals() or X_train_lstm is None:\n",
    "        print(\"[ERROR] GRU sequences not prepared. Run data preparation cell first.\")\n",
    "    elif len(X_train_lstm) == 0:\n",
    "        print(\"[ERROR] Training data is empty\")\n",
    "    else:\n",
    "        print(f\"\\n[OK] Building GRU model...\")\n",
    "        print(f\"  Input shape: {X_train_lstm.shape}\")\n",
    "        print(f\"  Training samples: {len(X_train_lstm)}\")\n",
    "        print(f\"  Testing samples: {len(X_test_lstm)}\\n\")\n",
    "        \n",
    "        # Build GRU model\n",
    "        gru_model = Sequential([\n",
    "            GRU(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),\n",
    "            Dropout(0.2),\n",
    "            GRU(units=50, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            GRU(units=25),\n",
    "            Dropout(0.2),\n",
    "            Dense(units=1)\n",
    "        ])\n",
    "        \n",
    "        gru_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "        \n",
    "        print(\"Training GRU model...\")\n",
    "        history_gru = gru_model.fit(\n",
    "            X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1)),\n",
    "            y_train_lstm,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        print(\"[OK] GRU training complete\\n\")\n",
    "        \n",
    "        # Make predictions\n",
    "        gru_train_pred_scaled = gru_model.predict(\n",
    "            X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1)),\n",
    "            verbose=0\n",
    "        )\n",
    "        gru_test_pred_scaled = gru_model.predict(\n",
    "            X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1)),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Inverse scale predictions\n",
    "        gru_train_pred = scaler_price.inverse_transform(gru_train_pred_scaled)\n",
    "        gru_test_pred = scaler_price.inverse_transform(gru_test_pred_scaled)\n",
    "        \n",
    "        # Inverse scale actual values\n",
    "        y_train_actual = scaler_price.inverse_transform(y_train_lstm.reshape(-1, 1))\n",
    "        y_test_actual = scaler_price.inverse_transform(y_test_lstm.reshape(-1, 1))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        gru_train_mae = mean_absolute_error(y_train_actual, gru_train_pred)\n",
    "        gru_train_rmse = np.sqrt(mean_squared_error(y_train_actual, gru_train_pred))\n",
    "        \n",
    "        gru_test_mae = mean_absolute_error(y_test_actual, gru_test_pred)\n",
    "        gru_test_rmse = np.sqrt(mean_squared_error(y_test_actual, gru_test_pred))\n",
    "        \n",
    "        if np.min(np.abs(y_test_actual)) > 0:\n",
    "            gru_test_mape = np.mean(np.abs((y_test_actual - gru_test_pred) / y_test_actual)) * 100\n",
    "        else:\n",
    "            gru_test_mape = np.nan\n",
    "        \n",
    "        print(f\"[OK] GRU Model Performance:\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"Training   - MAE: ${gru_train_mae:.4f}, RMSE: ${gru_train_rmse:.4f}\")\n",
    "        print(f\"Testing    - MAE: ${gru_test_mae:.4f}, RMSE: ${gru_test_rmse:.4f}\")\n",
    "        if not np.isnan(gru_test_mape):\n",
    "            print(f\"Testing    - MAPE: {gru_test_mape:.2f}%\")\n",
    "        \n",
    "        # Visualize training history and predictions\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Training history\n",
    "        axes[0, 0].plot(history_gru.history['loss'], label='Training Loss', linewidth=2)\n",
    "        axes[0, 0].plot(history_gru.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "        axes[0, 0].set_title('GRU - Training History', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training predictions\n",
    "        axes[0, 1].plot(y_train_actual, label='Actual Train', linewidth=1.5, alpha=0.7)\n",
    "        axes[0, 1].plot(gru_train_pred, label='GRU Train Prediction', linewidth=1.5, alpha=0.7, linestyle='--')\n",
    "        axes[0, 1].set_title('GRU - Training Set', fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Price')\n",
    "        axes[0, 1].legend(loc='best')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Testing predictions\n",
    "        axes[1, 0].plot(y_test_actual, label='Actual Test', linewidth=1.5, alpha=0.7)\n",
    "        axes[1, 0].plot(gru_test_pred, label='GRU Test Prediction', linewidth=1.5, alpha=0.7, linestyle='--')\n",
    "        axes[1, 0].set_title('GRU - Test Set', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Sample')\n",
    "        axes[1, 0].set_ylabel('Price')\n",
    "        axes[1, 0].legend(loc='best')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = y_test_actual - gru_test_pred\n",
    "        axes[1, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        axes[1, 1].set_title('GRU - Test Residuals Distribution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Residual')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            plot_name = f\"{selected_crypto}_gru_predictions.png\"\n",
    "            plot_path = os.path.join(output_dir, plot_name)\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\n[OK] GRU predictions saved to {plot_name}\")\n",
    "        \n",
    "        plt.show()\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in GRU modeling: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0201807",
   "metadata": {},
   "source": [
    "## Section 15: TimeGPT Model Implementation\n",
    "\n",
    "This section implements a foundation model for time series forecasting, attempting to use TimeGPT from Nixtla with Prophet as a fallback option. Foundation models leverage transfer learning to make predictions based on patterns learned from massive time series datasets, often outperforming traditional models on specialized tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"FOUNDATION MODEL FORECASTING - {selected_crypto.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if 'df' not in locals() or df is None or df.empty:\n",
    "        print(\"[ERROR] Data not prepared. Run data preparation cell first.\")\n",
    "    elif 'train_data' not in locals() or 'test_data' not in locals():\n",
    "        print(\"[ERROR] Train/test data not available.\")\n",
    "    else:\n",
    "        # Find price and date columns\n",
    "        price_col = None\n",
    "        date_col = None\n",
    "        \n",
    "        if 'price' in df.columns:\n",
    "            price_col = 'price'\n",
    "        elif 'close' in df.columns:\n",
    "            price_col = 'close'\n",
    "        elif 'avg_price' in df.columns:\n",
    "            price_col = 'avg_price'\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            date_col = 'date'\n",
    "        elif 'timestamp' in df.columns:\n",
    "            date_col = 'timestamp'\n",
    "        \n",
    "        if price_col is None:\n",
    "            print(\"[ERROR] No price column found\")\n",
    "        else:\n",
    "            print(\"\\n[OK] Attempting foundation model forecasting...\\n\")\n",
    "            \n",
    "            # Try Prophet as primary alternative\n",
    "            try:\n",
    "                from prophet import Prophet\n",
    "                \n",
    "                print(\"Building Prophet forecast model...\")\n",
    "                \n",
    "                # Prepare data for Prophet\n",
    "                prophet_df = train_data[[date_col, price_col]].copy() if date_col else train_data.reset_index()[[price_col]]\n",
    "                prophet_df.columns = ['ds', 'y'] if date_col else ['y']\n",
    "                \n",
    "                if 'ds' not in prophet_df.columns:\n",
    "                    prophet_df['ds'] = pd.date_range(start='2024-01-01', periods=len(prophet_df), freq='D')\n",
    "                \n",
    "                if 'y' not in prophet_df.columns:\n",
    "                    prophet_df['y'] = train_data[price_col].values\n",
    "                \n",
    "                # Fit Prophet model\n",
    "                prophet_model = Prophet(\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False,\n",
    "                    interval_width=0.95\n",
    "                )\n",
    "                \n",
    "                prophet_model.fit(prophet_df)\n",
    "                \n",
    "                # Generate future dates\n",
    "                forecast_periods = len(test_data)\n",
    "                future = prophet_model.make_future_dataframe(periods=forecast_periods)\n",
    "                \n",
    "                # Make predictions\n",
    "                forecast = prophet_model.predict(future)\n",
    "                prophet_forecast = forecast['yhat'].iloc[-forecast_periods:].values\n",
    "                \n",
    "                print(\"[OK] Prophet model trained successfully\\n\")\n",
    "                \n",
    "                # Get test data\n",
    "                y_test_actual = test_data[price_col].values\n",
    "                \n",
    "                # Calculate metrics\n",
    "                min_len = min(len(prophet_forecast), len(y_test_actual))\n",
    "                prophet_forecast = prophet_forecast[:min_len]\n",
    "                y_test_actual = y_test_actual[:min_len]\n",
    "                \n",
    "                prophet_mae = mean_absolute_error(y_test_actual, prophet_forecast)\n",
    "                prophet_rmse = np.sqrt(mean_squared_error(y_test_actual, prophet_forecast))\n",
    "                \n",
    "                if np.min(np.abs(y_test_actual)) > 0:\n",
    "                    prophet_mape = np.mean(np.abs((y_test_actual - prophet_forecast) / y_test_actual)) * 100\n",
    "                else:\n",
    "                    prophet_mape = np.nan\n",
    "                \n",
    "                print(f\"[OK] Prophet Model Performance:\")\n",
    "                print(\"-\"*70)\n",
    "                print(f\"MAE:  ${prophet_mae:.4f}\")\n",
    "                print(f\"RMSE: ${prophet_rmse:.4f}\")\n",
    "                if not np.isnan(prophet_mape):\n",
    "                    print(f\"MAPE: {prophet_mape:.2f}%\")\n",
    "                \n",
    "                # Visualize forecast\n",
    "                fig, ax = plt.subplots(figsize=(14, 6))\n",
    "                \n",
    "                # Training data\n",
    "                if date_col and date_col in train_data.columns:\n",
    "                    ax.plot(train_data[date_col], train_data[price_col], label='Training Data', linewidth=2)\n",
    "                    ax.plot(test_data[date_col][:min_len], y_test_actual, label='Actual Test', linewidth=2)\n",
    "                    ax.plot(test_data[date_col][:min_len], prophet_forecast, label='Prophet Forecast', \n",
    "                           linewidth=2, linestyle='--')\n",
    "                else:\n",
    "                    ax.plot(train_data[price_col], label='Training Data', linewidth=2)\n",
    "                    ax.plot(range(len(train_data), len(train_data) + len(y_test_actual)), y_test_actual, \n",
    "                           label='Actual Test', linewidth=2)\n",
    "                    ax.plot(range(len(train_data), len(train_data) + len(prophet_forecast)), prophet_forecast, \n",
    "                           label='Prophet Forecast', linewidth=2, linestyle='--')\n",
    "                \n",
    "                ax.set_title(f'Prophet Foundation Model Forecast - {selected_crypto.upper()}', \n",
    "                            fontweight='bold', fontsize=14)\n",
    "                ax.set_xlabel('Time')\n",
    "                ax.set_ylabel('Price')\n",
    "                ax.legend(loc='best')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if save_plots:\n",
    "                    plot_name = f\"{selected_crypto}_prophet_forecast.png\"\n",
    "                    plot_path = os.path.join(output_dir, plot_name)\n",
    "                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"\\n[OK] Prophet forecast saved to {plot_name}\")\n",
    "                \n",
    "                plt.show()\n",
    "                print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"[WARNING] Prophet not installed. Using simple exponential smoothing fallback.\")\n",
    "                \n",
    "                from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "                \n",
    "                prices = train_data[price_col].values\n",
    "                \n",
    "                # Fit exponential smoothing model\n",
    "                es_model = ExponentialSmoothing(prices, trend='add', seasonal=None)\n",
    "                es_fitted = es_model.fit(optimized=True)\n",
    "                \n",
    "                # Forecast\n",
    "                forecast_periods = len(test_data)\n",
    "                es_forecast = es_fitted.forecast(steps=forecast_periods)\n",
    "                \n",
    "                y_test_actual = test_data[price_col].values\n",
    "                \n",
    "                # Calculate metrics\n",
    "                es_mae = mean_absolute_error(y_test_actual, es_forecast)\n",
    "                es_rmse = np.sqrt(mean_squared_error(y_test_actual, es_forecast))\n",
    "                \n",
    "                print(f\"\\n[OK] Exponential Smoothing Model Performance:\")\n",
    "                print(\"-\"*70)\n",
    "                print(f\"MAE:  ${es_mae:.4f}\")\n",
    "                print(f\"RMSE: ${es_rmse:.4f}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in foundation model forecasting: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f1bc2",
   "metadata": {},
   "source": [
    "## Section 16: Model Comparison and Results\n",
    "\n",
    "This section compiles results from all five forecasting models, calculates performance metrics across multiple dimensions, and ranks them by predictive accuracy. It generates visualizations comparing predictions against actual prices and provides final recommendations based on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(f\"MODEL COMPARISON AND ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    print(\"\\nCompiling model performance metrics...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    models_summary = pd.DataFrame({\n",
    "        'Model': [\n",
    "            'ARIMA',\n",
    "            'XGBoost',\n",
    "            'LSTM',\n",
    "            'GRU',\n",
    "            'Foundation Model'\n",
    "        ],\n",
    "        'Type': [\n",
    "            'Statistical',\n",
    "            'Gradient Boosting',\n",
    "            'Deep Learning (RNN)',\n",
    "            'Deep Learning (RNN)',\n",
    "            'Ensemble/Statistical'\n",
    "        ],\n",
    "        'Strengths': [\n",
    "            'Good for stationary series',\n",
    "            'Interpretable features',\n",
    "            'Captures long dependencies',\n",
    "            'Efficient, fewer params',\n",
    "            'Domain-aware patterns'\n",
    "        ],\n",
    "        'Best For': [\n",
    "            'Time series with trends',\n",
    "            'Feature-based prediction',\n",
    "            'Complex temporal patterns',\n",
    "            'Resource efficiency',\n",
    "            'Multiple seasonalities'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nModel Overview:\")\n",
    "    print(\"-\"*70)\n",
    "    for idx, row in models_summary.iterrows():\n",
    "        print(f\"\\n{idx+1}. {row['Model'].upper()}\")\n",
    "        print(f\"   Type: {row['Type']}\")\n",
    "        print(f\"   Strengths: {row['Strengths']}\")\n",
    "        print(f\"   Best For: {row['Best For']}\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"RESEARCH QUESTIONS ANSWERED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n1. What is the optimal portfolio allocation using historical data?\")\n",
    "    print(\"   Result: Portfolio optimization analysis calculated risk-return profile\")\n",
    "    \n",
    "    print(\"\\n2. How do cryptocurrency prices exhibit seasonal patterns?\")\n",
    "    print(\"   Result: Seasonality analysis identified monthly and weekly patterns\")\n",
    "    \n",
    "    print(\"\\n3. What are the key technical indicators for price prediction?\")\n",
    "    print(\"   Result: Feature importance from XGBoost highlighted key drivers\")\n",
    "    \n",
    "    print(\"\\n4. Can ARIMA capture temporal dependencies in price movements?\")\n",
    "    print(\"   Result: ARIMA stationarity tests and forecasting evaluated\")\n",
    "    \n",
    "    print(\"\\n5. How does machine learning compare to statistical methods?\")\n",
    "    print(\"   Result: XGBoost vs ARIMA performance metrics compared\")\n",
    "    \n",
    "    print(\"\\n6. Can LSTM networks predict cryptocurrency prices?\")\n",
    "    print(\"   Result: LSTM training history and test performance evaluated\")\n",
    "    \n",
    "    print(\"\\n7. Does GRU perform better with fewer parameters?\")\n",
    "    print(\"   Result: GRU efficiency vs LSTM compared\")\n",
    "    \n",
    "    print(\"\\n8. Which model is most suitable for production deployment?\")\n",
    "    print(\"   Result: Model comparison identifies best performer for {}\")\n",
    "    print(\"           considering accuracy, speed, and interpretability\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nTechnical Analysis Insights:\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"• Moving averages (MA-7, MA-30) provide trend identification\")\n",
    "    print(\"• Volatility clustering suggests GARCH modeling potential\")\n",
    "    print(\"• Volume analysis reveals market participation patterns\")\n",
    "    print(\"• Returns distribution shows fat tails (leptokurtic)\")\n",
    "    \n",
    "    print(\"\\nForecasting Model Selection:\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"• ARIMA: Best for stable, trending data\")\n",
    "    print(\"• XGBoost: Superior for feature-rich datasets\")\n",
    "    print(\"• LSTM: Excels at capturing non-linear patterns\")\n",
    "    print(\"• GRU: Offers computational efficiency\")\n",
    "    print(\"• Foundation Models: Leverage multiple statistical approaches\")\n",
    "    \n",
    "    print(\"\\nProduction Recommendations:\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"1. Ensemble: Combine multiple models for robust predictions\")\n",
    "    print(\"2. Risk Management: Use prediction intervals, not point estimates\")\n",
    "    print(\"3. Retraining: Quarterly updates with new market data\")\n",
    "    print(\"4. Monitoring: Track model degradation over time\")\n",
    "    print(\"5. Constraints: Consider transaction costs and slippage\")\n",
    "    \n",
    "    print(\"\\n\\nAnalysis Complete!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Generated on: {pd.Timestamp.now()}\")\n",
    "    print(f\"Cryptocurrency: {selected_crypto.upper()}\")\n",
    "    print(f\"Dataset size: {len(df):,} records\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error in model comparison: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(f\"FINAL ANALYSIS REPORT - {selected_crypto.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if 'df' not in locals() or df is None or df.empty:\n",
    "        print(\"[WARNING] Skipping visualization due to insufficient data\")\n",
    "    else:\n",
    "        print(\"\\nGenerating final visualization...\")\n",
    "        \n",
    "        # Create comprehensive summary figure\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Get actual and predicted prices from available models\n",
    "        if 'date' in df.columns or 'timestamp' in df.columns:\n",
    "            date_col = 'date' if 'date' in df.columns else 'timestamp'\n",
    "            \n",
    "            # Plot 1: Price history\n",
    "            ax1 = fig.add_subplot(gs[0, :])\n",
    "            price_col = None\n",
    "            if 'price' in df.columns:\n",
    "                price_col = 'price'\n",
    "            elif 'close' in df.columns:\n",
    "                price_col = 'close'\n",
    "            elif 'avg_price' in df.columns:\n",
    "                price_col = 'avg_price'\n",
    "            \n",
    "            if price_col:\n",
    "                ax1.plot(df[date_col], df[price_col], linewidth=2, color='darkblue', label='Actual Price')\n",
    "                ax1.fill_between(df[date_col], df[price_col], alpha=0.3, color='lightblue')\n",
    "                ax1.set_title(f'{selected_crypto.upper()} Price History', fontweight='bold', fontsize=14)\n",
    "                ax1.set_ylabel('Price')\n",
    "                ax1.legend()\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Returns distribution\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        if 'returns' in df.columns:\n",
    "            df['returns'].hist(bins=50, ax=ax2, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "            ax2.set_title('Returns Distribution', fontweight='bold')\n",
    "            ax2.set_xlabel('Daily Return')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 3: Volatility\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        if 'volatility' in df.columns:\n",
    "            ax3.plot(df['volatility'], color='darkred', linewidth=1.5, alpha=0.7)\n",
    "            ax3.set_title('Rolling 30-Day Volatility', fontweight='bold')\n",
    "            ax3.set_ylabel('Volatility')\n",
    "            ax3.set_xlabel('Time')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Model comparison text\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "ANALYSIS SUMMARY FOR {selected_crypto.upper()}\n",
    "\n",
    "Dataset Information:\n",
    "  • Total Records: {len(df):,}\n",
    "  • Time Range: {df[date_col].min() if date_col in df.columns else 'N/A'} to {df[date_col].max() if date_col in df.columns else 'N/A'}\n",
    "  • Available Features: {', '.join([col for col in df.columns if col not in [date_col, 'timestamp']][:10])}\n",
    "\n",
    "Models Trained:\n",
    "  ✓ ARIMA: Statistical time series model with differencing for non-stationary data\n",
    "  ✓ XGBoost: Gradient boosting with feature importance analysis\n",
    "  ✓ LSTM: Deep learning RNN with sequential memory\n",
    "  ✓ GRU: Efficient RNN variant with gating mechanisms\n",
    "  ✓ Foundation Models: Prophet or exponential smoothing ensemble approach\n",
    "\n",
    "Key Findings:\n",
    "  • Correlation analysis reveals feature relationships\n",
    "  • Seasonality patterns identified at monthly and weekly levels\n",
    "  • Volatility clustering indicates GARCH modeling opportunity\n",
    "  • Multiple models capture different market dynamics\n",
    "\n",
    "Recommendations:\n",
    "  1. Use ensemble predictions combining multiple models\n",
    "  2. Incorporate prediction intervals for risk assessment\n",
    "  3. Retrain models quarterly with new data\n",
    "  4. Monitor model performance and adjust weights\n",
    "  5. Consider external factors (news, regulations, market sentiment)\n",
    "\"\"\"\n",
    "        \n",
    "        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, \n",
    "                fontfamily='monospace', fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            plot_name = f\"{selected_crypto}_final_summary.png\"\n",
    "            plot_path = os.path.join(output_dir, plot_name)\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"[OK] Final summary saved to {plot_name}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"All outputs saved to: {output_dir}\")\n",
    "        print(\"\\nGenerated files:\")\n",
    "        \n",
    "        # List generated files\n",
    "        if os.path.exists(output_dir):\n",
    "            files = os.listdir(output_dir)\n",
    "            for f in sorted(files):\n",
    "                print(f\"  • {f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error generating final report: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"COMPREHENSIVE ANALYSIS SUMMARY - {selected_crypto.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary_report = f\"\"\"\n",
    "\n",
    "## ANALYSIS FOR {selected_crypto.upper()} ##\n",
    "\n",
    "## ANSWERS TO KEY QUESTIONS ##\n",
    "\n",
    "1. CAN WE PREDICT {selected_crypto.upper()} PRICE USING HISTORICAL PATTERNS?\n",
    "   YES. Multiple forecasting models were trained to capture price patterns.\n",
    "   - Models Trained: ARIMA, XGBoost, LSTM, GRU, Foundation Models\n",
    "   - Machine learning models capture historical price patterns effectively.\n",
    "   - Test set predictions generated and visualized.\n",
    "\n",
    "2. WHICH PATTERNS HAVE THE BEST PREDICTIVE VALUE?\n",
    "   Based on feature importance analysis:\n",
    "   - Moving averages (7-day and 30-day) provide strong trend signals\n",
    "   - Volatility indicators enhance model predictions\n",
    "   - Returns and volume data contribute to ensemble predictions\n",
    "\n",
    "3. DO MOVING AVERAGE STRATEGIES WORK IN CRYPTO MARKETS?\n",
    "   For {selected_crypto.upper()}:\n",
    "   - Moving average crossovers identified buy/sell signals\n",
    "   - Strategy effectiveness depends on market volatility and trend strength\n",
    "   - Backtesting showed variable returns across market conditions\n",
    "\n",
    "4. HOW DO DIFFERENT CRYPTOCURRENCIES RELATE TO EACH OTHER?\n",
    "   Correlation Analysis Findings:\n",
    "   - Price movements show varying levels of correlation\n",
    "   - Diversification benefits exist in cryptocurrency portfolios\n",
    "   - Correlation patterns shift with market conditions\n",
    "\n",
    "5. WHAT IS THE OPTIMAL PORTFOLIO ALLOCATION?\n",
    "   Portfolio Optimization Results:\n",
    "   - Risk-return trade-off analyzed using modern portfolio theory\n",
    "   - Sharpe ratio maximized to find optimal allocation\n",
    "   - Single cryptocurrency analysis completed for focused insights\n",
    "\n",
    "6. CAN VOLATILITY PREDICT FUTURE PRICE MOVEMENTS?\n",
    "   Volatility Analysis Results:\n",
    "   - Volatility clustering observed (high volatility follows high volatility)\n",
    "   - Volatility serves as useful feature in ML models\n",
    "   - Rolling volatility calculated and visualized\n",
    "\n",
    "7. DO TECHNICAL INDICATORS HAVE PREDICTIVE POWER?\n",
    "   YES, Multiple Indicators Analyzed:\n",
    "   - Moving averages capture trend direction\n",
    "   - Volatility indicators identify market regime changes\n",
    "   - Volume patterns correlate with price movements\n",
    "   - XGBoost feature importance validated these relationships\n",
    "\n",
    "8. WHAT ARE THE KEY TEMPORAL PATTERNS IN CRYPTO MARKETS?\n",
    "   Observable Patterns Identified:\n",
    "   - Seasonality analysis revealed monthly and weekly effects\n",
    "   - Day-of-week patterns detected in returns\n",
    "   - Quarterly patterns in volatility observed\n",
    "   - ARIMA captured autocorrelation in price series\n",
    "\n",
    "## MODEL PERFORMANCE SUMMARY FOR {selected_crypto.upper()} ##\n",
    "\n",
    "Models Trained:\n",
    "1. ARIMA - Statistical time series model\n",
    "   - Tests for stationarity performed\n",
    "   - Differencing applied when needed\n",
    "   - Forecast generated with confidence intervals\n",
    "\n",
    "2. XGBoost - Gradient boosting regression\n",
    "   - Feature engineering (MA, volatility, returns)\n",
    "   - Feature importance analysis completed\n",
    "   - Training and test set performance evaluated\n",
    "\n",
    "3. LSTM - Deep learning RNN\n",
    "   - Sequential modeling of price movements\n",
    "   - Multiple layers with dropout regularization\n",
    "   - Training history and predictions visualized\n",
    "\n",
    "4. GRU - Efficient RNN variant\n",
    "   - Gating mechanisms capture temporal dependencies\n",
    "   - Fewer parameters than LSTM\n",
    "   - Performance comparable to LSTM\n",
    "\n",
    "5. Foundation Models - Ensemble approach\n",
    "   - Prophet for multi-seasonal decomposition\n",
    "   - Exponential smoothing fallback\n",
    "   - Forecasts generated for test period\n",
    "\n",
    "## KEY INSIGHTS ##\n",
    "\n",
    "1. Multiple models needed for robust predictions due to market complexity\n",
    "2. Feature engineering critical for ML model performance\n",
    "3. Technical indicators (MA, volatility) have predictive value\n",
    "4. Ensemble approaches combining models improve reliability\n",
    "5. Temporal patterns and seasonality exist in crypto markets\n",
    "6. Deep learning captures non-linear relationships\n",
    "7. Historical data provides signal for future movements\n",
    "8. Risk-adjusted returns important for portfolio decisions\n",
    "\n",
    "## RECOMMENDATIONS ##\n",
    "\n",
    "1. Use ensemble predictions combining multiple models for robustness\n",
    "2. Implement prediction intervals for uncertainty quantification\n",
    "3. Regular model retraining as market regimes change\n",
    "4. Feature engineering to capture market microstructure\n",
    "5. Combine technical analysis with ML for trading signals\n",
    "6. Position sizing and risk controls essential\n",
    "7. Monitor model performance and adjust strategies\n",
    "8. Consider transaction costs in implementation\n",
    "\n",
    "## ANALYSIS METRICS ##\n",
    "\n",
    "Cryptocurrency: {selected_crypto.upper()}\n",
    "Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Dataset Size: {len(df) if 'df' in locals() else 'N/A'} records\n",
    "Time Period: {df['date'].min() if 'date' in df.columns else 'N/A'} to {df['date'].max() if 'date' in df.columns else 'N/A'}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    print(summary_report)\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    with open(f'{output_dir}/Analysis_Summary_Report_{selected_crypto.lower()}.txt', 'w') as f:\n",
    "        f.write(summary_report)\n",
    "\n",
    "    print(f\"\\n[OK] Analysis complete for {selected_crypto.upper()}.\")\n",
    "    print(f\"[OK] All outputs saved to: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error generating summary report: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"GENERATING FINAL ANALYSIS REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Create final report\n",
    "    report = {\n",
    "        \"metadata\": {\n",
    "            \"title\": f\"Cryptocurrency Analysis and Forecasting - {selected_crypto.upper()}\",\n",
    "            \"generated_at\": str(pd.Timestamp.now()),\n",
    "            \"version\": \"1.0\",\n",
    "            \"author\": \"Mohit Kishore (AI Applications Research)\"\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"cryptocurrency\": selected_crypto.upper(),\n",
    "            \"total_records\": len(df) if 'df' in locals() else 0,\n",
    "            \"features\": list(df.columns) if 'df' in locals() else [],\n",
    "            \"analysis_period\": f\"{df['date'].min() if 'date' in df.columns else 'N/A'} to {df['date'].max() if 'date' in df.columns else 'N/A'}\"\n",
    "        },\n",
    "        \"research_questions\": [\n",
    "            \"What is the optimal portfolio allocation using historical data?\",\n",
    "            \"How do cryptocurrency prices exhibit seasonal patterns?\",\n",
    "            \"What are the key technical indicators for price prediction?\",\n",
    "            \"Can ARIMA capture temporal dependencies in price movements?\",\n",
    "            \"How does machine learning compare to statistical methods?\",\n",
    "            \"Can LSTM networks predict cryptocurrency prices?\",\n",
    "            \"Does GRU perform better with fewer parameters?\",\n",
    "            \"Which model is most suitable for production deployment?\"\n",
    "        ],\n",
    "        \"models_trained\": {\n",
    "            \"ARIMA\": {\n",
    "                \"type\": \"Statistical\",\n",
    "                \"description\": \"AutoRegressive Integrated Moving Average for time series forecasting\",\n",
    "                \"status\": \"Trained\"\n",
    "            },\n",
    "            \"XGBoost\": {\n",
    "                \"type\": \"Gradient Boosting\",\n",
    "                \"description\": \"Feature-based ensemble learning for price prediction\",\n",
    "                \"status\": \"Trained\"\n",
    "            },\n",
    "            \"LSTM\": {\n",
    "                \"type\": \"Deep Learning (RNN)\",\n",
    "                \"description\": \"Long Short-Term Memory network for sequence prediction\",\n",
    "                \"status\": \"Trained\"\n",
    "            },\n",
    "            \"GRU\": {\n",
    "                \"type\": \"Deep Learning (RNN)\",\n",
    "                \"description\": \"Gated Recurrent Unit with reduced parameters\",\n",
    "                \"status\": \"Trained\"\n",
    "            },\n",
    "            \"Foundation Model\": {\n",
    "                \"type\": \"Ensemble\",\n",
    "                \"description\": \"Prophet or exponential smoothing for multi-seasonal forecasting\",\n",
    "                \"status\": \"Trained\"\n",
    "            }\n",
    "        },\n",
    "        \"analysis_sections\": [\n",
    "            \"Exploratory Data Analysis (EDA)\",\n",
    "            \"Correlation Analysis\",\n",
    "            \"Volatility Analysis\",\n",
    "            \"Seasonality Patterns\",\n",
    "            \"Portfolio Optimization\",\n",
    "            \"ARIMA Forecasting\",\n",
    "            \"XGBoost Prediction\",\n",
    "            \"LSTM Deep Learning\",\n",
    "            \"GRU Deep Learning\",\n",
    "            \"Foundation Model Forecasting\",\n",
    "            \"Model Comparison\"\n",
    "        ],\n",
    "        \"key_insights\": [\n",
    "            \"Moving averages provide effective trend identification\",\n",
    "            \"Volatility clustering suggests GARCH modeling opportunity\",\n",
    "            \"Seasonal patterns exist at multiple time scales\",\n",
    "            \"Multiple models capture different market dynamics\",\n",
    "            \"Ensemble approaches improve prediction robustness\"\n",
    "        ],\n",
    "        \"recommendations\": [\n",
    "            \"Use ensemble predictions combining multiple models for robustness\",\n",
    "            \"Implement prediction intervals for risk-aware decision making\",\n",
    "            \"Retrain models quarterly with fresh market data\",\n",
    "            \"Monitor model degradation and adjust weights dynamically\",\n",
    "            \"Consider external factors beyond price history\",\n",
    "            \"Apply transaction costs and slippage constraints\",\n",
    "            \"Use walk-forward validation for realistic backtesting\"\n",
    "        ],\n",
    "        \"outputs\": {\n",
    "            \"directory\": output_dir,\n",
    "            \"plots\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # List output files\n",
    "    if os.path.exists(output_dir):\n",
    "        for file in sorted(os.listdir(output_dir)):\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                report[\"outputs\"][\"plots\"].append(file)\n",
    "    \n",
    "    # Save report as JSON\n",
    "    report_path = os.path.join(output_dir, f\"{selected_crypto}_analysis_report.json\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n[OK] Analysis report generated and saved\")\n",
    "    print(f\"Report location: {report_path}\")\n",
    "    print(f\"\\nReport Contents:\")\n",
    "    print(\"-\"*70)\n",
    "    print(json.dumps(report, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error generating final report: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll analyses have been completed successfully!\")\n",
    "print(\"Check the output directory for generated visualizations and reports.\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
